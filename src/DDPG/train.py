import copy
import datetime
import logging
import math
import os
import random
from argparse import Namespace
from collections import deque
from enum import Enum
from typing import Optional, Union

import numpy as np
import torch
from tqdm import tqdm

from DDPG.ReplayBuffer import ReplayBuffer
from DDPG.agent import Agent
from DDPG.env import RoadEnv
from DDPG.utils.my_timer import MyTimer
from DDPG.utils.reward_utils import *
from geo import Road, Building, Region
from utils import io_utils

__version__ = 312
logging.basicConfig(level=logging.INFO)

'''æé†’äº‹é¡¹ï¼š
 - è¿™é‡Œç»™å®šçš„æ•°å€¼å’Œå‚æ•°åç§°ä¼šè¢«guiè§£æå¹¶æ˜¾ç¤ºåœ¨çª—å£ä¸­
 - ä»¥ä¸‹argsç»™å®šçš„é»˜è®¤æ•°å€¼ï¼Œå¦‚æœæ˜¯floatçš„è¯ä¸€å®šè¦å†™æˆfloatçš„å½¢å¼ï¼Œä¾‹å¦‚0.0ï¼Œä¸è¦å†™æˆ0ï¼Œå¦åˆ™ä¼šè¢«guiè§£æä¸ºint
'''
DEFAULT_ENV_ARGS = Namespace(
    data_path=r'../data/VirtualEnv/only_roads.bin',  # åœ°å›¾æ•°æ®æ–‡ä»¶ï¼Œ binæ ¼å¼ã€‚å¦‚æœç•™ç©ºï¼Œåˆ™ä½¿ç”¨å·²ç»åŠ è½½çš„æ•°æ®
    num_agents=1,  # æ™ºèƒ½ä½“æ•°é‡
    region_min=None,  # Noneè¡¨ç¤ºè‡ªåŠ¨è®¡ç®—èŒƒå›´
    region_max=None,
    observation_img_size=(128, 128),
    observation_view_size=(400.0, 400.0),
    observation_center=(200.0, 200.0),  # æ‘„åƒæœºè§†è§’çš„ä¸­å¿ƒç‚¹åæ ‡
    still_mode=False,  # æ˜¯å¦å›ºå®šæ‘„åƒæœºè§†è§’
    action_step_range=(15, 40),#(20,60)
    max_episode_step=50,  # æ¯ä¸ªæ™ºèƒ½ä½“æœ€å¤šèµ°å¤šå°‘æ­¥
)

DEFAULT_TRAIN_ARGS = Namespace(
    path_actor='',  # ç¼“å­˜çš„actorçš„pthæ–‡ä»¶
    path_critic='',  # ç¼“å­˜çš„criticçš„pthæ–‡ä»¶
    path_cnn='',  # ç¼“å­˜çš„cnnçš„pthæ–‡ä»¶
    path_buffer='',  # ç¼“å­˜çš„bufferæ–‡ä»¶
    nb_actions=2,  # actionçš„ç»´åº¦
    warmup_epochs=10000,  # é¢„çƒ­è½®æ¬¡
    train_epochs=10001,  # è®­ç»ƒè½®æ¬¡
    sigma=1.000,  # è‡ªç”±æ¢ç´¢åˆå§‹å€¼
    sigma_decay=0.9995,  # è‡ªç”±æ¢ç´¢çš„è¡°å‡å€¼
    actor_lr=1e-4,  #
    critic_lr=1e-3,  #
    tau=0.001,  #
    gamma=0.9,  #
    batch_size=64,  #
    buffer_capacity=10000,  # ç»éªŒç©ºé—´æœ€å¤§å®¹é‡
    save_interval=10000,  # ä¿å­˜é—´éš”
    log_folder='',  # ä¿å­˜pthçš„æ–‡ä»¶å¤¹ï¼Œ ç•™ç©ºåˆ™å°†åœ¨logsæ–‡ä»¶å¤¹è‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªå½“å‰æ—¶é—´çš„æ–‡ä»¶å¤¹ç”¨äºä¿å­˜
    save_warmup_buffer=False  # åœ¨warmupå®Œæˆåä¿å­˜replay buffer
)

DEFAULT_GUI_ARGS = Namespace(
    headless=False  # æ— å¤´æ¨¡å¼ï¼Œ é»˜è®¤ä¸ºTrueï¼Œ è¦åœ¨GUIä¸­æ˜¾ç¤ºï¼Œè¯·æ”¹ä¸ºFalse
)

_shared_data: Optional[dict] = None
_sigma = 1.0
_return_queue = deque(maxlen=10)  # è¿™é‡Œçš„é•¿åº¦æ˜¯ç”¨æ¥å¹³å‡çš„é•¿åº¦
_mean_return_arr: Optional[np.ndarray] = None  # åœ¨trainä¸­æ ¹æ®éœ€è¦çš„é•¿åº¦åˆ›å»º
_critic_loss_queue = deque(maxlen=10)  # è¿™é‡Œçš„é•¿åº¦æ˜¯ç”¨æ¥å¹³å‡çš„é•¿åº¦
_mean_critic_loss_arr: Optional[np.ndarray] = None  # åœ¨trainä¸­æ ¹æ®éœ€è¦çš„é•¿åº¦åˆ›å»º
_actor_loss_queue = deque(maxlen=10)
_mean_actor_loss_arr: Optional[np.ndarray] = None


class TrainMode(Enum):
    WARMUP = 0
    TRAIN = 1


def train(env_args=DEFAULT_ENV_ARGS,
          train_args=DEFAULT_TRAIN_ARGS,
          gui_args=DEFAULT_GUI_ARGS,
          shared_data=None):
    """
    :param env_args:
    :param train_args:
    :param gui_args:
    :param shared_data: å…±äº«ä¿¡æ¯
    :return:
    """
    # init global data
    global _shared_data, _sigma
    global _return_queue, _mean_return_arr
    global _critic_loss_queue, _mean_critic_loss_arr
    global _actor_loss_queue, _mean_actor_loss_arr
    _shared_data = shared_data
    _sigma = train_args.sigma
    _return_queue.clear()
    _critic_loss_queue.clear()
    _actor_loss_queue.clear()
    _mean_return_arr = np.zeros(train_args.train_epochs, dtype=np.float32)
    _mean_critic_loss_arr = np.zeros(train_args.train_epochs, dtype=np.float32)
    _mean_actor_loss_arr = np.zeros(train_args.train_epochs, dtype=np.float32)

    # init shared data
    if _shared_data is None:
        _shared_data = {'slow_ratio': 1,  # æ”¾æ…¢é€Ÿåº¦
                        'save_buffer': False  # è®¾ä¸ºTrueæ—¶ä¿å­˜å½“å‰replay bufferï¼Œ éšå³è®¾ä¸ºFalse
                        }
    # _shared_data ä¸­ä¸€å®šæœ‰ slow_ratio
    if 'slow_ratio' not in _shared_data:
        _shared_data['slow_ratio'] = 1
    # _shared_dataä¸­ä¸€å®šæœ‰ save_buffer
    if 'save_buffer' not in _shared_data:
        _shared_data['save_buffer'] = False

    MyTimer.register_shared_data(_shared_data)

    # init map data
    with MyTimer('init_data_time', level=0):
        if env_args.data_path != '':
            data = io_utils.load_data(env_args.data_path)
            Building.data_to_buildings(data)
            Region.data_to_regions(data)
            Road.data_to_roads(data)
        yield None
    # create env
    with MyTimer('init_env_time', level=0):
        print(f'ğŸ¯creating env...')
        env = RoadEnv(num_road_agents=env_args.num_agents,
                      max_episode_step=env_args.max_episode_step,
                      region_min=env_args.region_min,
                      region_max=env_args.region_max,
                      observation_img_size=env_args.observation_img_size,
                      observation_view_size=env_args.observation_view_size,
                      still_mode=env_args.still_mode,
                      observation_center=env_args.observation_center,
                      action_step_range=env_args.action_step_range,
                      headless=gui_args.headless,
                      shared_data=_shared_data,
                      )
        yield None
    # create agent
    with MyTimer('init_agent_time', level=0):
        action_space_bound = torch.tensor(env.action_space_bound)
        action_space_boundMove = torch.tensor(env.action_space_boundMove)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f'creating replay buffer...')
        replay_buffer: ReplayBuffer = ReplayBuffer(capacity=train_args.buffer_capacity)
        reward_info_buffer: deque = deque(maxlen=train_args.buffer_capacity)

        agent: Agent = Agent(nb_actions=train_args.nb_actions,
                             action_space_bound=action_space_bound,
                             action_space_boundMove=action_space_boundMove,
                             actor_lr=train_args.actor_lr,
                             critic_lr=train_args.critic_lr,
                             tau=train_args.tau,
                             gamma=train_args.gamma,
                             device=device)
        yield None

    # load state
    with MyTimer('load_state_time', level=0):
        if train_args.path_actor != '':
            agent.actor.load_state_dict(torch.load(train_args.path_actor))
        if train_args.path_critic != '':
            agent.critic.load_state_dict(torch.load(train_args.path_critic))
        if train_args.path_cnn != '':
            agent.cnn.load_state_dict(torch.load(train_args.path_cnn))
        if train_args.path_buffer != '':
            print(f'loading buffer from {train_args.path_buffer}')
            replay_buffer.load(train_args.path_buffer)

    if train_args.log_folder == '':
        train_args.log_folder = f'logs/{datetime.datetime.now().strftime("%Y%m%d%H%M%S")}'
    _shared_data['log_folder'] = train_args.log_folder
    print(f'log files will be saved to {os.path.abspath(train_args.log_folder)}')

    # region [ğŸ¤–warm_up]
    with MyTimer('warmup_time', level=0):
        _shared_data['stage'] = 'warmup'
        for epoch in tqdm(range(train_args.warmup_epochs)):  # å…ˆç©nè½®
            with MyTimer('epoch_time', level=1):
                gen = epoch_generator(epoch, TrainMode.WARMUP, env, agent, train_args, env_args, gui_args, replay_buffer,
                                      reward_info_buffer)
                try:
                    while True:
                        next(gen)
                        yield None
                except StopIteration:
                    pass
    # save replay buffer
    if train_args.save_warmup_buffer:
        print(f'saving warmup_buffer...')
        save_replay_buffer(replay_buffer, env_args, train_args, gui_args, reward_info_buffer)
    # endregion
    # region [ğŸš€train]
    with MyTimer('train_time', level=0):
        _shared_data['stage'] = 'train'
        for epoch in tqdm(range(train_args.train_epochs)):
            with MyTimer('epoch_time', level=1):
                gen = epoch_generator(epoch, TrainMode.TRAIN, env, agent, train_args,env_args, gui_args, replay_buffer,
                                      reward_info_buffer)
                try:
                    while True:
                        next(gen)
                        yield None
                except StopIteration:
                    pass
            save_model(epoch, train_args, agent)
            # endregion
    # endregion
    print(f'ğŸ˜Šcomplete')


def epoch_generator(epoch: int, mode: TrainMode, env: RoadEnv, agent: Agent, train_args: Namespace, env_args, gui_args,
                    replay_buffer: ReplayBuffer, reward_info_buffer: deque):
    """
    æ¯è½®epochè¿è¡Œçš„å‡½æ•°, å¯¹äºwarmupå’Œtrainéƒ½æ˜¯ä¸€æ ·çš„ï¼Œé€šè¿‡è®¾ç½®modeæ¥åŒºåˆ†

    :param epoch:ç°åœ¨æ˜¯ç¬¬å‡ è½®
    :param mode:è®­ç»ƒæ¨¡å¼TrainMode.TRAIN  æˆ– TrainMode.WARMUP
    :param env:
    :param agent:
    :param train_args:
    :param replay_buffer:
    :param reward_info_buffer:
    :return:
    """
    global _sigma
    # region [warm_up/train episode]
    _shared_data['epoch'] = epoch
    with MyTimer('env_reset_time', level=2):
        state = env.reset()
    done = env.done_in_ndarray_format
    # play game
    with MyTimer('total_play_time', level=2):

        buffer_list: list[list] = []
        """
        buffer_list æ˜¯ä¸€ä¸ªä½œç”¨åŸŸèŒƒå›´ä¸ºæ¯ä¸€è½®epochçš„å˜é‡ï¼Œ
        
        å…¶ç”¨äºä¸´æ—¶è®°å½•æ¯ä¸€è½®æ™ºèƒ½ä½“çš„replay bufferä¿¡æ¯ï¼Œæ”¶é›†å®Œä¸€æ•´è½®çš„bufferä¿¡æ¯åç»è¿‡rewardåç§»å†äº¤ç”±replay_bufferä¿å­˜ã€‚
        
        æ™ºèƒ½ä½“åœ¨è¿è¡Œçš„æœŸé—´ä¼šåŠ¨æ€ç»™buffer_listæ·»åŠ å€¼ã€‚
        
        buffer_listçš„ç»“æ„ä¸º step_idx: [state, action, reward, next_state, done, Done] : ...
        
        """

        reward_info_list: list[dict[int:dict[int:float]]] = []
        """
        reward_info_listæ˜¯ä¸€ä¸ªä½œç”¨åŸŸèŒƒå›´ä¸ºæ¯ä¸€è½®epochçš„å˜é‡ï¼Œ

        å…¶è®°å½•äº†æ¯ä¸€ä¸ªstepä¸­ï¼Œæ¯ä¸ªagentçš„å„é¡¹rewardçš„å€¼ï¼Œ å…·ä½“çš„rewardåç§°ç”±reward_agent.pyæ–‡ä»¶å®šä¹‰ã€‚

        æ™ºèƒ½ä½“åœ¨è¿è¡Œçš„æœŸé—´ä¼šåŠ¨æ€ç»™reward_info_listæ·»åŠ å€¼ã€‚

        reward_info_listçš„ä¸€èˆ¬ç»“æ„ä¸º int step_idx: int agent_idx: int reward_name: float reward_value
        [
            step_idx : dict{
                agent_idx : dict{
                    reward_name: reward_value,
                    ...
                    sum: reward_value,  # ç‰¹æ®Šé¡¹
                    final_reward_name: final_reward_value,
                    ...
                    final_reward_shift: value,  # ç‰¹æ®Šé¡¹
                    final_reward_weight: value,  # ç‰¹æ®Šé¡¹
                    }
                }
                ...
            }
            ...
        ]
        """

        gen = agent_play(epoch, env, mode, agent, done, state, train_args, replay_buffer, buffer_list, reward_info_list)
        """
        å®šä¹‰ä¸€ä¸ªagent playçš„generator
        
        ä½¿ç”¨ next(gen)æ¥è°ƒç”¨è¯¥è¿­ä»£å™¨ï¼Œ è®©agentèµ°ä¸€æ­¥ï¼Œ å½“agentæ²¡æœ‰ä¸‹ä¸€æ­¥æ—¶ï¼Œä¼šæŠ›å‡º StopIteration Exception
        
        å…¶ä¸­å‚æ•°ä¸­çš„buffer_listä¸reward_info_listä¼šåœ¨agent playçš„è¿‡ç¨‹ä¸­åŠ¨æ€æ›´æ–°
        """
        try:
            while True:
                next(gen)
                yield None
        except StopIteration:
            pass
    with MyTimer('final_reward_time', level=2):
        final_reward = env.calculate_final_reward()  # ç»“æŸä¸€å±€æ¸¸æˆåï¼Œè®¡ç®—æœ€ç»ˆçš„reward
    final_reward_info: dict[int:dict[str:float]] = _shared_data['final_reward_info']  # æ‹¿åˆ°å…·ä½“çš„final reward info
    """
    final_reward_infoä¿å­˜äº†final rewardçš„å…·ä½“æ¯ä¸€é¡¹çš„å€¼
    
    env.calculate_final_reward()ä¼šæ›´æ–°shared_dataçš„'final_reward_info'é¡¹ï¼Œ
    
    final_reward_infoè¿™ä¸ªkeyçš„åç§°æ˜¯ç”±envå®šä¹‰çš„
    
    final_reward_infoçš„ç»“æ„ä¸º agent_idx: reward_name: reward_value
    """
    shift_reward(final_reward, final_reward_info,buffer_list, reward_info_list)  # åç§»åŸæœ‰çš„bufferä¸­çš„rewardä¿¡æ¯
    add_to_replay_buffer(buffer_list, replay_buffer)  # å°†åç§»åçš„bufferæ•°æ®æ·»åŠ åˆ°replay_bufferä¸­
    add_to_reward_info_buffer(reward_info_list, reward_info_buffer)  # å°†reward infoæ•°æ®æ·»åŠ åˆ°reward_info_bufferä¸­
    if mode == TrainMode.TRAIN:
        _sigma *= train_args.sigma_decay  # é™ä½sigma
        _shared_data['sigma'] = _sigma
        _return_queue.append(calculate_episode_return(buffer_list))  # å°†è¿™ä¸€è½®çš„rewardå’Œæ·»åŠ åˆ°return_list
        _mean_return_arr[epoch] = sum(_return_queue) / len(_return_queue)
        n = epoch + 1
        if n < 100:
            _shared_data['mean_return_list'] = _mean_return_arr[:n]
            _shared_data['mean_critic_loss'] = _mean_critic_loss_arr[:n]
            _shared_data['mean_actor_loss'] = _mean_actor_loss_arr[:n]
        else:
            sample_size = 100
            step = (n - 1) // (sample_size - 1)  # è®¡ç®—å¹³å‡é—´éš”ï¼ˆé™¤äº†æœ€åä¸€ä¸ªå…ƒç´ ï¼‰
            sampled_mean_return_arr = _mean_return_arr[:n:step]
            sampled_critic_loss_arr = _mean_critic_loss_arr[:n:step]
            sampled_actor_loss_arr = _mean_actor_loss_arr[:n:step]
            _shared_data['mean_return_list'] = np.append(sampled_mean_return_arr, _mean_return_arr[epoch])
            _shared_data['mean_critic_loss'] = np.append(sampled_critic_loss_arr, _mean_critic_loss_arr[epoch])
            _shared_data['mean_actor_loss'] = np.append(sampled_actor_loss_arr, _mean_actor_loss_arr[epoch])

    if _shared_data['save_buffer']:
        print(f'saving buffer...')
        save_replay_buffer(replay_buffer, env_args, train_args, gui_args, reward_info_buffer)
        _shared_data['save_buffer'] = False

    with MyTimer('yield_time', level=2):
        for i in range(_shared_data['slow_ratio']):
            yield None
    # endregion


def agent_play(epoch, env, mode, agent, done, state, train_args, replay_buffer, out_data_list: list,
               out_reward_info_list: list[dict[int:dict[str:float]]]):
    out_data_list.clear()
    out_reward_info_list.clear()
    critic_loss = 0
    actor_loss = 0
    while True:
        # region [agent step]
        # take action
        with MyTimer('agent_action_time', level=3):
            action = agent_take_action(env, done, agent, state, train_args)
        # env step
        with MyTimer('env_step_time', level=3):
            next_state, reward, done, Done = env.step(action)
        # add to replay buffer  (out_data_buffer)
        out_data_list.append([state, action, reward, next_state, done, Done])
        reward_info: dict[int:dict[str:float]] = _shared_data['reward_info']  # agent_idx: reward_name : reward_value
        out_reward_info_list.append(copy.copy(reward_info))  # è¿™é‡Œä¸€å®šè¦æ³¨æ„ç”¨copyï¼Œä¸ç„¶åŠ å…¥çš„ä¼šæ˜¯dictçš„åœ°å€ï¼Œä¼šéšç€dictçš„æ”¹å˜è€Œæ”¹å˜
        state = next_state  # state changed, should be returned and updated

        # agent_update(mode, env, agent, replay_buffer, train_args)
        _shared_data['episode_step'] = env.episode_step

        if mode == TrainMode.TRAIN:
            # update agent
            critic_loss, actor_loss = agent_update(env, agent, replay_buffer, train_args)  # éšæœºé‡‡æ ·batch sizeä¸ªæ•°æ®å–‚ç»™agentå­¦ä¹ 

        if Done: break

        with MyTimer('yield_time', level=3):
            for i in range(_shared_data['slow_ratio']):
                yield None

        # endregion
    _critic_loss_queue.append(critic_loss)
    _actor_loss_queue.append(actor_loss)
    _mean_critic_loss_arr[epoch] = sum(_critic_loss_queue) / len(_critic_loss_queue)
    _mean_actor_loss_arr[epoch] = sum(_actor_loss_queue) / len(_actor_loss_queue)


def agent_update(env, agent, replay_buffer, train_args):
    if replay_buffer.size() <= train_args.batch_size:
        return 0, 0
    # start update
    with MyTimer('agent_update_time', level=3):
        s, a, r, ns, d, D = replay_buffer.sample(train_args.batch_size)
        transition_dict = {
            'states': s,
            'actions': a,
            'rewards': r,
            'next_states': ns,
            'dones': d,
            'Dones': D,
        }
        for i in range(0, env.num_road_agents):
            critic_loss, actor_loss = agent.update(i, transition_dict)
    # end update
    return critic_loss, actor_loss


def agent_take_action(env, done, agent, state, train_args):
    action_list = []
    for i in range(env.num_road_agents):
        if done[i]:
            action = np.zeros((1, 2))
        else:
            action = agent.take_action(state)
            noise = np.random.randn(train_args.nb_actions)  # åé¢å¯èƒ½æ¢æˆouå™ªå£°ï¼Œå¢åŠ å™ªå£°å¼ºåº¦é€æ¸è¡°å‡çš„åŠŸèƒ½
            # alpha = random.uniform(0, _sigma)
            action[0][0] = (action[0][0] + noise[0] * _sigma + 3) % 2.0 - 1  # -1:1 -> -2:2 -> 1:5 -> 0:2 -> -1:1
            action[0][1] = (action[0][1] + noise[1] * _sigma + 3) % 2.0 - 1
        action_list.append(action)
    action = np.array(action_list).reshape(-1, 2)
    _shared_data['action_0'] = action[0]
    return action


def shift_reward(final_reward: np.ndarray,
                 final_reward_info: dict[int:dict[str:float]],
                 buffer_list: list[list],
                 reward_info_list: list[dict[int:dict[str, float]]],
                 ):
    """
    åç§»åŸæœ‰çš„bufferä¸­çš„rewardä¿¡æ¯

    å…¶ä¸­buffer_listä¸reward_info_listä¸­çš„å†…å®¹ä¼šè¢«åŠ¨æ€ä¿®æ”¹ï¼Œä¸éœ€è¦è¿”å›

    ç¤ºä¾‹ï¼š

    # >>> done:bool    = buffer_list[step: int][4][agent_index: int][0]

    # >>> reward:float = buffer_list[step: int][2][agent_index: int][0]

    :param final_reward: æœ€ç»ˆçš„rewardï¼Œ å½¢çŠ¶ä¸º(n, 1)ï¼Œ nè¡¨ç¤ºnä¸ªæ™ºèƒ½ä½“
    :param buffer_list: æ¯ä¸€è½®epochçš„ä¸´æ—¶buffer_list,
    å…¶å†…å®¹æ ¼å¼ä¸º step_idx : [state, action, reward, next_state, done, Done] : ...
    :param reward_info_list:å…¶å†…å®¹æ ¼å¼ä¸º step_idx: agent_idx: reward_name: reward_value
    :param final_reward_info: å…¶å†…å®¹æ ¼å¼ä¸º agent_idx: reward_name: reward_value
    :return:
    """

    num_steps = len(buffer_list)
    rewards_per_agent = []  # [agent_index][step]
    for agent_idx in range(len(final_reward)):
        # for each agent
        rewards = [buffer_list[step][2][agent_idx][0] for step in range(num_steps)]
        dones = [int(buffer_list[step][4][agent_idx][0]) for step in range(num_steps)]
        reward_shift = final_reward[agent_idx][0]

        first_done_step = len(dones) - sum(dones)
        x = 0  # x is the distance from last step
        for step in range(first_done_step, -1, -1):
            # from last step to first step
            weight = assign_reward_weights(x)  # calculate weight
            shift_value = weight * reward_shift
            # shift rewards
            rewards[step] = rewards[step] + shift_value

            # add final_reward_info to reward_info_list
            reward_info_list[step][agent_idx][FINAL_REWARD_SHIFT] = shift_value
            reward_info_list[step][agent_idx][FINAL_REWARD_WEIGHT] = weight
            for reward_name, reward_value in final_reward_info[agent_idx].items():
                reward_info_list[step][agent_idx][reward_name] = reward_value * weight
            x += 1
        rewards_per_agent.append(rewards)

    rewards_per_agent = np.array(rewards_per_agent)
    rewards_per_step = np.transpose(rewards_per_agent)
    for step in range(num_steps):
        buffer_list[step][2] = rewards_per_step[step].reshape((-1, 1))


def assign_reward_weights(x):
    """
    rewardæƒé‡åˆ†é…æ–¹æ³•
    :param x: ç¦»æœ€åä¸€æ­¥çš„è·ç¦»ã€‚ æœ€åä¸€æ­¥x=0ï¼Œ ç¦»æœ€åä¸€æ­¥è¶Šè¿‘xè¶Šå°
    :return: rewardåˆ†é…æƒé‡
    """
    return 1.0 / (x + 1.0)  # 1/x+1


def calculate_episode_return(data_buffer):
    # (epoch, agent, 1)
    rewards_arr: np.ndarray = np.array([data_buffer[epoch][2] for epoch in range(len(data_buffer))])
    return np.sum(rewards_arr)


def add_to_replay_buffer(buffer_list: list[list], replay_buffer: ReplayBuffer):
    for item in buffer_list:
        replay_buffer.add(*item)


def add_to_reward_info_buffer(reward_info_list, reward_info_buffer):
    for item in reward_info_list:
        reward_info_buffer.append(item)


def save_model(epoch, train_args, agent):
    if epoch % train_args.save_interval != 0:
        return
    if epoch <= 0:
        return
    with MyTimer('save_time', level=1):
        if not os.path.exists(train_args.log_folder):
            os.makedirs(train_args.log_folder)
        torch.save(agent.actor.state_dict(),
                   os.path.join(train_args.log_folder, f'actor_{epoch}.pth'))
        torch.save(agent.critic.state_dict(),
                   os.path.join(train_args.log_folder, f'critic_{epoch}.pth'))
        torch.save(agent.cnn.state_dict(),
                   os.path.join(train_args.log_folder, f'cnn_{epoch}.pth'))


def save_replay_buffer(replay_buffer: ReplayBuffer, env_args, train_args, gui_args, reward_info_buffer):
    import getpass
    time_str = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    meta_data = {
        'env_args': env_args,
        'train_args': train_args,
        'gui_args': gui_args,
        'reward_info_buffer': reward_info_buffer,
        'user_name': getpass.getuser(),
        'save_time': time_str,
        'train_version': __version__
    }
    path = f'logs/replay_buffer_{replay_buffer.size()}_{time_str}.pkl.gz'
    replay_buffer.save(path, meta_data=meta_data)
    print(f'replay buffer saved to {path}')
